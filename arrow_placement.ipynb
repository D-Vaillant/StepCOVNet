{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dropout, Dense, Flatten, BatchNormalization, LSTM, Input, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(mypath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "def get_notes(notes_path):\n",
    "    all_song_notes = []\n",
    "    file_names = get_file_names(notes_path)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_notes = []\n",
    "        with open(notes_path + file_name, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            file_notes = np.asarray([x.replace(\"\\n\", \"\").split(\" \") for x in lines[3:]]).astype(\"float32\")\n",
    "            #file_notes = file_notes[: -1] # removing last note since there is not next note\n",
    "        all_song_notes.append(file_notes)\n",
    "    return np.asarray(all_song_notes)\n",
    "\n",
    "\n",
    "notes_by_song = get_notes(\"timings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24406, 5, 4), (24406, 3), (24406, 15))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_rep(arrow_values):\n",
    "    return (((arrow_values.astype(int)[:,None] & (1 << np.arange(4)))) > 0).astype(int)\n",
    "\n",
    "\n",
    "def create_tokens(timings):\n",
    "    tokens = np.zeros((timings.shape[0], 3))\n",
    "    tokens[0][0] = 1 # set start token\n",
    "    next_note_token = np.append(timings[1:] - timings[:-1], np.asarray([0]))\n",
    "    prev_note_token = np.append(np.asarray([0]),  next_note_token[: -1])\n",
    "    tokens[:, 1] = prev_note_token.reshape(1, -1)\n",
    "    tokens[:, 2] = next_note_token.reshape(1, -1)\n",
    "    return tokens[:-1].astype(\"float32\")\n",
    "\n",
    "\n",
    "def get_notes_ngram(binary_steps, lookback):\n",
    "    padding = np.zeros((look_back, binary_steps.shape[1]))\n",
    "    data_w_padding = np.append(padding, binary_steps, axis = 0)\n",
    "    return np.asarray(list(ngrams(data_w_padding, look_back)))[:-1]\n",
    "\n",
    "\n",
    "def data_prep(notes_by_song, lookback = 5):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder(categories='auto', sparse = False).fit(np.asarray(range(1, 16)).reshape(-1, 1))\n",
    "    \n",
    "    all_arrows = []\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for notes in notes_by_song:\n",
    "        binary_notes = get_binary_rep(notes[:, 0][:-1])\n",
    "        \n",
    "        notes_ngram = get_notes_ngram(binary_notes, lookback)\n",
    "        tokens = create_tokens(notes[:, 1])\n",
    "        labels = encoder.transform(notes[:, 0][:-1].reshape(-1, 1))\n",
    "        \n",
    "        all_arrows.append(notes_ngram)\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    return np.concatenate(all_arrows), np.concatenate(all_tokens), np.concatenate(all_labels)\n",
    "\n",
    "lookback = 256\n",
    "all_arrows, all_tokens, all_labels = data_prep(notes_by_song, lookback = lookback)\n",
    "all_arrows.shape, all_tokens.shape, all_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "arrows_train, arrows_test, labels_train, labels_test = \\\n",
    "    train_test_split(all_arrows, all_labels, test_size=0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "tokens_train, tokens_test, _, _ = \\\n",
    "    train_test_split(np.expand_dims(all_tokens, axis = 1), all_labels, random_state = 42, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(arrows_shape, token_shape, output_shape, lookback, silent = True):\n",
    "    arrows = Input(shape = (arrows_shape[1], arrows_shape[2],))\n",
    "    tokens = Input(shape = (token_shape[1], token_shape[2],))\n",
    "    \n",
    "    x = LSTM(lookback, kernel_initializer='glorot_normal', return_sequences = True)(arrows)\n",
    "    x = LSTM(lookback)(x)\n",
    "    x = Model(inputs = arrows, outputs = x)\n",
    "    \n",
    "    y = LSTM(lookback, kernel_initializer='glorot_normal', return_sequences = True)(tokens)\n",
    "    y = LSTM(lookback)(y)\n",
    "    y = Model(inputs = tokens, outputs = y)\n",
    "    \n",
    "    combined = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = Dense(256, kernel_initializer='glorot_normal', activation = \"relu\")(combined)\n",
    "    z = Dropout(0.5)(z)\n",
    "    z = Dense(output_shape[1], activation = \"softmax\")(z)\n",
    "    \n",
    "    model = Model(inputs = [x.input, y.input], outputs = z)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    \n",
    "    if not silent: \n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 5, 4)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 5, 256)       267264      input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 1, 256)       266240      input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  (None, 256)          525312      lstm_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 256)          525312      lstm_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           lstm_45[0][0]                    \n",
      "                                                                 lstm_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 256)          131328      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 15)           3855        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,719,311\n",
      "Trainable params: 1,719,311\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build(arrows_train.shape, tokens_train.shape, labels_train.shape, lookback = lookback, silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19524 samples, validate on 4882 samples\n",
      "Epoch 1/100\n",
      "19524/19524 [==============================] - 13s 653us/step - loss: 1.8301 - acc: 0.2567 - val_loss: 1.6300 - val_acc: 0.3058\n",
      "Epoch 2/100\n",
      "19524/19524 [==============================] - 6s 310us/step - loss: 1.4527 - acc: 0.4452 - val_loss: 1.2988 - val_acc: 0.5248\n",
      "Epoch 3/100\n",
      "19524/19524 [==============================] - 6s 326us/step - loss: 1.3186 - acc: 0.5221 - val_loss: 1.2682 - val_acc: 0.5459\n",
      "Epoch 4/100\n",
      "19524/19524 [==============================] - 6s 321us/step - loss: 1.2725 - acc: 0.5378 - val_loss: 1.2113 - val_acc: 0.5623\n",
      "Epoch 5/100\n",
      "19524/19524 [==============================] - 6s 314us/step - loss: 1.2466 - acc: 0.5438 - val_loss: 1.1986 - val_acc: 0.5621\n",
      "Epoch 6/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 1.2217 - acc: 0.5512 - val_loss: 1.1965 - val_acc: 0.5643\n",
      "Epoch 7/100\n",
      "19524/19524 [==============================] - 6s 310us/step - loss: 1.2026 - acc: 0.5603 - val_loss: 1.1967 - val_acc: 0.5666\n",
      "Epoch 8/100\n",
      "19524/19524 [==============================] - 6s 309us/step - loss: 1.1863 - acc: 0.5616 - val_loss: 1.1525 - val_acc: 0.5723\n",
      "Epoch 9/100\n",
      "19524/19524 [==============================] - 6s 311us/step - loss: 1.1712 - acc: 0.5671 - val_loss: 1.1676 - val_acc: 0.5809\n",
      "Epoch 10/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 1.1518 - acc: 0.5695 - val_loss: 1.1437 - val_acc: 0.5795\n",
      "Epoch 11/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 1.1326 - acc: 0.5790 - val_loss: 1.1248 - val_acc: 0.5858\n",
      "Epoch 12/100\n",
      "19524/19524 [==============================] - 6s 312us/step - loss: 1.1157 - acc: 0.5809 - val_loss: 1.1271 - val_acc: 0.5854\n",
      "Epoch 13/100\n",
      "19524/19524 [==============================] - 6s 312us/step - loss: 1.0991 - acc: 0.5863 - val_loss: 1.1659 - val_acc: 0.5828\n",
      "Epoch 14/100\n",
      "19524/19524 [==============================] - 6s 314us/step - loss: 1.0810 - acc: 0.5930 - val_loss: 1.1170 - val_acc: 0.5873\n",
      "Epoch 15/100\n",
      "19524/19524 [==============================] - 6s 312us/step - loss: 1.0633 - acc: 0.6019 - val_loss: 1.1299 - val_acc: 0.5899\n",
      "Epoch 16/100\n",
      "19524/19524 [==============================] - 6s 318us/step - loss: 1.0401 - acc: 0.6028 - val_loss: 1.1253 - val_acc: 0.5858\n",
      "Epoch 17/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 1.0278 - acc: 0.6115 - val_loss: 1.1412 - val_acc: 0.5823\n",
      "Epoch 18/100\n",
      "19524/19524 [==============================] - 6s 314us/step - loss: 0.9999 - acc: 0.6216 - val_loss: 1.1467 - val_acc: 0.5795\n",
      "Epoch 19/100\n",
      "19524/19524 [==============================] - 6s 314us/step - loss: 0.9805 - acc: 0.6250 - val_loss: 1.1420 - val_acc: 0.5854\n",
      "Epoch 20/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 0.9382 - acc: 0.6381 - val_loss: 1.1589 - val_acc: 0.5873\n",
      "Epoch 21/100\n",
      "19524/19524 [==============================] - 6s 316us/step - loss: 0.9090 - acc: 0.6478 - val_loss: 1.1709 - val_acc: 0.5868\n",
      "Epoch 22/100\n",
      "19524/19524 [==============================] - 6s 315us/step - loss: 0.8928 - acc: 0.6571 - val_loss: 1.1999 - val_acc: 0.5848\n",
      "Epoch 23/100\n",
      "19524/19524 [==============================] - 6s 315us/step - loss: 0.8774 - acc: 0.6592 - val_loss: 1.2122 - val_acc: 0.5864\n",
      "Epoch 24/100\n",
      "19524/19524 [==============================] - 6s 313us/step - loss: 0.8617 - acc: 0.6650 - val_loss: 1.2117 - val_acc: 0.5875\n"
     ]
    }
   ],
   "source": [
    "batch_size = lookback\n",
    "model = build(arrows_train.shape, tokens_train.shape, labels_train.shape, lookback = lookback, silent = True)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)]\n",
    "\n",
    "history = model.fit([arrows_train, tokens_train], \n",
    "                    labels_train,\n",
    "                    validation_data=([arrows_test, tokens_test], labels_test),\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size= batch_size,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
